{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFuDdRCbsMx3"
      },
      "source": [
        "FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mkCDfrgjeDx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random as rnd\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from matplotlib import pyplot\n",
        "import matplotlib\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "def getDataset(small,lenght,default=0):\n",
        "    dataset_path = 'https://raw.githubusercontent.com/TM111/Banking-Dataset_Marketing-Targets_Dataspaces-Exam/main/bank_dataset_full.csv'\n",
        "    if(small==1):\n",
        "        dataset_path = 'https://raw.githubusercontent.com/TM111/Banking-Dataset_Marketing-Targets_Dataspaces-Exam/main/bank_dataset_small.csv'\n",
        "        n = len(pd.read_csv(dataset_path,delimiter=\";\"))\n",
        "        skip=[]\n",
        "        if(n-lenght>1):\n",
        "            skip = sorted(rnd.sample(range(1,n+1),n-lenght)) \n",
        "        dataset = pd.read_csv(dataset_path,delimiter=\";\", skiprows=skip)\n",
        "    else:\n",
        "        dataset=pd.read_csv(dataset_path,delimiter=\";\")\n",
        "\n",
        "    dataset.drop([\"duration\",\"month\",\"day_of_week\"], axis=1, inplace=True) # elimino attributi\n",
        "    if(default==0):\n",
        "      dataset.drop([\"default\"], axis=1, inplace=True) # elimino default\n",
        "      \n",
        "    dataset=deleteMissingValues(dataset, \"unknown\")\n",
        "    print(\"\\nDataset size \"+str(len(dataset))+\"\\n\")\n",
        "    \n",
        "    for i in range(2,rnd.randint(3,6)):#mescolo un po il dataset\n",
        "        for j in range(2,rnd.randint(3,6)):\n",
        "          dataset = dataset.sample(frac=1).reset_index(drop=True) \n",
        "    return dataset\n",
        " \n",
        "def monthToSeason(ds):\n",
        "    d = {\"dec\": \"autumn_winter\",\"jan\": \"autumn_winter\",\"feb\": \"autumn_winter\",\"mar\": \"spring_summer\",\"apr\": \"spring_summer\",\"may\": \"spring_summer\",\n",
        "         \"jun\": \"spring_summer\",\"jul\": \"spring_summer\",\"aug\": \"spring_summer\",\"sep\": \"autumn_winter\",\"oct\": \"autumn_winter\",\"nov\": \"autumn_winter\",}\n",
        "    for i in range(0,len(ds)):\n",
        "        ds.at[i,\"month\"]=d[ds.iloc[i][\"month\"]]\n",
        "    return ds\n",
        "    \n",
        "    \n",
        "def deleteMissingValues(ds,att):\n",
        "    dataset=ds\n",
        "    indexRows=[]\n",
        "    index=-1\n",
        "    for row in dataset.iloc:\n",
        "        index=index+1\n",
        "        percentage=\"Delete missing values: \"+str(int(100*index/len(dataset)))+\"%\"\n",
        "        sys.stdout.write('\\r'+percentage)\n",
        "        if(\"unknown\" in row.values):\n",
        "            indexRows.append(index)\n",
        "    sys.stdout.write('\\r'+\"                                             \"+'\\r')\n",
        "    dataset.drop(indexRows , inplace=True)  #elimino missing values\n",
        "    dataset.reset_index(drop=True, inplace=True) \n",
        "    return dataset\n",
        " \n",
        "def encodingDataset(dataset):\n",
        "    dataset=labelEncoder(dataset,[\"housing\",\"loan\",\"contact\",\"y\"])\n",
        "    dataset=OneHotEncoder(dataset,[\"job\",\"marital\",\"education\",\"poutcome\"]) \n",
        "    return dataset\n",
        "    \n",
        "    \n",
        "def OneHotEncoder(ds,attributes):\n",
        "    dataset=ds\n",
        "    for att in attributes:\n",
        "        one_hot = pd.get_dummies(dataset[att])\n",
        "        dataset = dataset.drop(att,axis = 1)\n",
        "        dataset = dataset.join(one_hot)\n",
        "        for i in range(0,len(one_hot.columns)):\n",
        "            dataset = dataset.rename(columns={one_hot.columns[i]: att+\"_\"+one_hot.columns[i]})\n",
        "    attributes=[]\n",
        "    for att in dataset.columns:\n",
        "        if(att!=\"y\"):\n",
        "            attributes.append(att)\n",
        "    attributes.append(\"y\")\n",
        "    dataset= dataset[attributes]\n",
        "    return dataset\n",
        " \n",
        "def labelEncoder(ds,attributes):\n",
        "    dataset=ds\n",
        "    le = LabelEncoder()\n",
        "    for att in attributes:\n",
        "        dataset[att]=le.fit(dataset[att]).transform(dataset[att])\n",
        "    return dataset\n",
        " \n",
        "def getOccurrences(ds,attribute,normalize=0,order=0):\n",
        "    column=ds[attribute]\n",
        "    values={}\n",
        "    for v in column:\n",
        "        if(v not in values):\n",
        "            if(attribute==\"euribor3m\"):\n",
        "                v=str(round(float(v),2))\n",
        "            values[v]=[0,0]\n",
        "    \n",
        "    for i in range(0,len(ds)):\n",
        "        value=ds.iloc[i][attribute]\n",
        "        c=ds.iloc[i][\"y\"]\n",
        "        if(attribute==\"euribor3m\"):\n",
        "                value=str(round(float(value),2))\n",
        "        if(c==\"no\" or c==0):\n",
        "            values[value]=[values[value][0]+1,values[value][1]]\n",
        "        else:\n",
        "            values[value]=[values[value][0],values[value][1]+1]\n",
        " \n",
        "    keys=list(values.keys())\n",
        " \n",
        "    no_list=[]\n",
        "    yes_list=[]\n",
        "    for k in values.keys():\n",
        "        sum=values[k][0]+values[k][1]\n",
        "        if(normalize==0):\n",
        "            sum=1\n",
        "        no_list.append(values[k][0]/sum)\n",
        "        yes_list.append(values[k][1]/sum)\n",
        "    if(order==1):\n",
        "        if(normalize==1):\n",
        "            for i in range(0,len(yes_list)-1):\n",
        "                for j in range(0,len(yes_list)-1):\n",
        "                    if(yes_list[j]>yes_list[j+1]):\n",
        "                        yes_list[j], yes_list[j+1] = yes_list[j+1], yes_list[j]\n",
        "                        no_list[j], no_list[j+1] = no_list[j+1], no_list[j]\n",
        "                        keys[j], keys[j+1] = keys[j+1], keys[j]\n",
        "        else:\n",
        "            for i in range(0,len(yes_list)-1):\n",
        "                for j in range(0,len(yes_list)-1):\n",
        "                    if(yes_list[j]+no_list[j]<yes_list[j+1]+no_list[j+1]):\n",
        "                        yes_list[j], yes_list[j+1] = yes_list[j+1], yes_list[j]\n",
        "                        no_list[j], no_list[j+1] = no_list[j+1], no_list[j]\n",
        "                        keys[j], keys[j+1] = keys[j+1], keys[j]\n",
        "    if(order==2):\n",
        "        for i in range(0,len(yes_list)-1):\n",
        "            for j in range(0,len(yes_list)-1):\n",
        "                a=keys[j].split(\"\\n\")[0]\n",
        "                b=keys[j+1].split(\"\\n\")[0]\n",
        "                try:\n",
        "                    if(float(a)>float(b)):\n",
        "                        yes_list[j], yes_list[j+1] = yes_list[j+1], yes_list[j]\n",
        "                        no_list[j], no_list[j+1] = no_list[j+1], no_list[j]\n",
        "                        keys[j], keys[j+1] = keys[j+1], keys[j]\n",
        "                except:\n",
        "                    if(a>b):\n",
        "                        yes_list[j], yes_list[j+1] = yes_list[j+1], yes_list[j]\n",
        "                        no_list[j], no_list[j+1] = no_list[j+1], no_list[j]\n",
        "                        keys[j], keys[j+1] = keys[j+1], keys[j]\n",
        "    return keys,no_list,yes_list\n",
        "    \n",
        "def getStatistic(ds,attribute):\n",
        "    column=ds[attribute]\n",
        "    statistics={}\n",
        "    statistics[\"Min\"]=min(column)\n",
        "    statistics[\"1st Q.\"]=np.quantile(column, 0.25)\n",
        "    statistics[\"Median\"]=np.quantile(column, 0.5)\n",
        "    statistics[\"Mean\"]=(sum(column)/len(column))\n",
        "    statistics[\"Std\"]=np.std(column, axis=0)\n",
        "    statistics[\"3st Q.\"]=np.quantile(column, 0.75)\n",
        "    statistics[\"Max\"]=max(column)\n",
        "    return statistics  \n",
        "    \n",
        "def drawStatisticsTable(data,columns,rows):\n",
        "    matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "    data_tmp=np.array(data).T.tolist()\n",
        "    for i in range(0,len(rows)):\n",
        "        for j in range(0,len(columns)):\n",
        "            if(int(data_tmp[i][j])==data_tmp[i][j]):\n",
        "                data_tmp[i][j]=int(data_tmp[i][j])\n",
        "            else:\n",
        "                data_tmp[i][j]=round(data_tmp[i][j],3)\n",
        "    Rcolors = plt.cm.BuPu(np.linspace(0.3, 0.3, len(rows)))\n",
        "    Ccolors = plt.cm.BuPu(np.linspace(0.3, 0.3, len(columns)))\n",
        "    cell_text = []\n",
        "    for row in data_tmp:\n",
        "        cell_text.append(row)\n",
        "    the_table = plt.table(cellText=cell_text,\n",
        "                      rowLabels=rows,\n",
        "                      rowColours=Rcolors,\n",
        "                      colColours=Ccolors,\n",
        "                      colLabels=columns,\n",
        "                      cellLoc=\"center\",\n",
        "                      loc='center')\n",
        "    the_table.scale(1.2, 3)\n",
        "    ax = plt.gca()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    plt.box(on=None)\n",
        "    plt.show()\n",
        "\n",
        "def plot_cm(cm,target_names,title='Confusion matrix',cmap=None,normalize=False):\n",
        "    matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "    precision = round(float(cm.item(3))/float((cm.item(1)+cm.item(3))),2)\n",
        "    recall=round(float(cm.item(3))/float((cm.item(2)+cm.item(3))),2)\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(\"\")\n",
        "    plt.colorbar()\n",
        "    plt.grid(False)\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.rcParams.update({'font.size': 19})\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\n'.format())\n",
        "    plt.show()\n",
        "\n",
        "def plot_FI(clf,columns,num):\n",
        "  matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "  importance = clf.feature_importances_\n",
        "  height=[]\n",
        "  bars=[]\n",
        "  for i,v in enumerate(importance):\n",
        "    height.append(v)\n",
        "    bars.append(columns[i])\n",
        "  \n",
        "  for i in range(0,len(height)):\n",
        "    for j in range(0,len(height)):\n",
        "      if(height[i]>height[j]):\n",
        "        height[i], height[j] = height[j], height[i]\n",
        "        bars[i], bars[j] = bars[j], bars[i]\n",
        "\n",
        "  height=height[:num][::-1]\n",
        "  bars=bars[:num][::-1]\n",
        "  y_pos = np.arange(len(bars))\n",
        "  plt.figure(figsize=(4, 3))\n",
        "  plt.rcParams.update({'font.size': 10})\n",
        "  plt.barh(y_pos, height)\n",
        "  plt.yticks(y_pos, bars)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_learning_curve_model(X, Y, model,smote):\n",
        "    matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "    cv = 5\n",
        "    train_sizes=[]\n",
        "    train_scores=[]\n",
        "    test_scores=[]\n",
        "    partitions=15\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)\n",
        "    for i in range(1,partitions+1):\n",
        "        percentage=\"Percentage: \"+str(int(100*(i)/partitions))+\"%\"\n",
        "        sys.stdout.write('\\r'+percentage)\n",
        "        if(i==partitions):\n",
        "            Xtrain=X_train\n",
        "            Ytrain=y_train\n",
        "        \n",
        "            Xtest=X_test\n",
        "            Ytest=y_test\n",
        "        else:\n",
        "            Xtrain=X_train[:int(len(X_train)/partitions)*i]\n",
        "            Ytrain=y_train[:int(len(y_train)/partitions)*i]\n",
        "        \n",
        "            Xtest=X_test[:int(len(X_test)/partitions)*i]\n",
        "            Ytest=y_test[:int(len(y_test)/partitions)*i]\n",
        "        try:\n",
        "          if(smote):\n",
        "              Xtrain_smote, Ytrain_smote = SMOTE().fit_resample(Xtrain, Ytrain)\n",
        "              clf = model.fit(Xtrain_smote, Ytrain_smote)\n",
        "          else:\n",
        "              clf = model.fit(Xtrain, Ytrain)\n",
        "          pred_train = clf.predict(Xtrain)\n",
        "          train_scores.append([f1_score(Ytrain, pred_train),f1_score(Ytrain, pred_train)])\n",
        "          pred_test = clf.predict(Xtest)\n",
        "          test_scores.append([f1_score(Ytest, pred_test),f1_score(Ytest, pred_test)])\n",
        "          train_sizes.append(len(Xtrain))\n",
        "        except Exception as e: \n",
        "          print(\" \"+str(e))\n",
        "          continue\n",
        "    sys.stdout.write('\\r')\n",
        "    plt.figure(figsize=(7, 7))\n",
        "    plt.tick_params(axis='y', labelleft='off', labelright='on')\n",
        "    plt.xlabel(\"Training examples\",fontsize=16)\n",
        "    plt.ylabel(\"F1 Score\",fontsize=16)\n",
        "    train_size=np.linspace(.1, 1.0, 15)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std  = np.std(train_scores, axis=1)\n",
        "    test_scores_mean  = np.mean(test_scores, axis=1)\n",
        "    test_scores_std   = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"tab:blue\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"tab:orange\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"tab:blue\",label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"tab:orange\",label=\"Test score\")\n",
        "    plt.legend(loc=\"best\",fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "def plot_ROC(clf,y_train,y_test):\n",
        "  matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "  le = LabelEncoder()\n",
        "  y_train=le.fit(y_train).transform(y_train)\n",
        "  y_test=le.fit(y_test).transform(y_test)\n",
        "  y_scores = clf.predict_proba(X_test)\n",
        "  fpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  plt.figure(figsize=(6,6))\n",
        "  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  plt.legend(loc = 'lower right',fontsize=15)\n",
        "  plt.plot([0, 1], [0, 1],'r--')\n",
        "  plt.xlim([0, 1])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.ylabel('True Positive Rate',fontsize=16)\n",
        "  plt.xlabel('False Positive Rate',fontsize=16)\n",
        "  plt.grid(False)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRL5ZQhPkiX"
      },
      "source": [
        "DATASET ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gW_WUUPkC5"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#IF YOU WANT A GRAPH, ACTIVATE \"plot\" VARIABLE\n",
        "\n",
        "#LOAD DATASET\n",
        "small=1      #set to 1 if you want the small dataset for a faster analysis\n",
        "lenght=4000     #the lenght of small dataset\n",
        "\n",
        "dataset=getDataset(small,lenght, 1)\n",
        "\n",
        "numeric_attributes=[\"age\",\"campaign\",\"pdays\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\n",
        "categorical_attributes=[\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"poutcome\"]\n",
        "\n",
        "#CORRLATION MATRIX\n",
        "plot=1\n",
        "df=dataset[numeric_attributes]\n",
        "if(plot):\n",
        "    print(\"\\nCORRLATION MATRIX\")\n",
        "    corr_df = df.corr(method='pearson')\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    sns.set(font_scale=0.7)\n",
        "    sns.heatmap(corr_df, annot=True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#SCATTER MATRIX\n",
        "plot=0\n",
        "if(plot):\n",
        "  print(\"\\nSCATTER MATRIX\")\n",
        "  sns.set(font_scale=0.7)\n",
        "  g=sns.pairplot(df,plot_kws={\"s\": 4})\n",
        "  g.fig.set_size_inches(10,10)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#STATISTICS TABLE\n",
        "plot=1\n",
        "if(plot):\n",
        "  print(\"\\nSTATISTICS TABLE\")\n",
        "  data=[]\n",
        "  plt.figure(figsize=(12, 4))\n",
        "  for att in df.columns:\n",
        "    data.append(list(getStatistic(df,att).values()))\n",
        "  drawStatisticsTable(data, list(df.columns), list(getStatistic(df,\"age\").keys()))\n",
        "\n",
        "\n",
        "#BOXPLOT and DENSITY  (numeric variables, change \"att\")\n",
        "plot=0\n",
        "numeric_attributes=[\"age\",\"campaign\",\"pdays\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\n",
        "att=numeric_attributes[0]\n",
        "if(plot):\n",
        "    print(\"\\nBOXPLOT \"+att)\n",
        "    flierprops = dict(markerfacecolor='0.5', markersize=10,linestyle='none')\n",
        "    sns.set(font_scale=2)\n",
        "    plt.figure(figsize=(8,7))\n",
        "    sns.boxplot(x=\"y\", y=att, linewidth=3,data=dataset, flierprops=flierprops,\n",
        "                hue=\"y\")\n",
        "    plt.legend(loc='upper center')\n",
        "    plt.xlabel(\"\")\n",
        "    plt.show()\n",
        "if(plot):\n",
        "    print(\"\\nDENSITY \"+att)\n",
        "    plt.figure(figsize=(8,7))\n",
        "    ax=sns.kdeplot(data=dataset, x=att, hue=\"y\",multiple=\"stack\")\n",
        "    ax.yaxis.tick_right()\n",
        "    ax.legend_.set_bbox_to_anchor((0.05, 0.95))\n",
        "    ax.legend_._set_loc(2)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#BAR CHARTS    (categorical variables, change \"att\")\n",
        "plot=0\n",
        "categorical_attributes=[\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"poutcome\"]\n",
        "att=categorical_attributes[0]\n",
        "if(plot):\n",
        "    print(\"\\nBAR CHARTS \"+att)\n",
        "    plt.figure(figsize=(15,6))\n",
        "    fontsize=12\n",
        "    margin=0.09\n",
        "    height=4\n",
        "    total = len(dataset)\n",
        "    ax=sns.countplot(x=att, data=dataset, hue=\"y\")\n",
        "    ax.set(ylabel='#samples')\n",
        "    plt.legend(loc='upper right')\n",
        "    for p in ax.patches:\n",
        "        ax.annotate('{:.2f}%'.format(100*p.get_height()/total), (p.get_x()+margin, p.get_height()+height)\n",
        "                    ,fontsize=fontsize)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\",\n",
        "                       fontsize=fontsize+4)\n",
        "    plt.show()\n",
        "if(plot):\n",
        "    print(\"\")\n",
        "    wid=0.4\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    keys,no_list,yes_list=getOccurrences(dataset,att,1,1)\n",
        "    plt.xticks(rotation=40)\n",
        "    plt.bar(keys, no_list,width=wid)\n",
        "    plt.bar(keys, yes_list, bottom = no_list,width=wid)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TktjYlf6UtF4"
      },
      "source": [
        "LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJxT-vmWjkeD"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from imblearn.over_sampling import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import *\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "#LOAD DATASET\n",
        "small=1      #set to 1 if you want to use the small dataset for a faster analysis\n",
        "lenght=1000     #the lenght of small dataset\n",
        "smote=1            #set to 1 if you want SMOTE\n",
        "dataset=getDataset(small,lenght)\n",
        " \n",
        "#ENCODING\n",
        "dataset=encodingDataset(dataset)\n",
        " \n",
        "#TRAIN, VAL e TEST\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, len(dataset.columns)-1].values\n",
        " \n",
        "test_ratio = 0.10\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
        "\n",
        "val_ratio = 0.15\n",
        "kf = KFold(n_splits=int((1-test_ratio)/val_ratio))\n",
        "kf.get_n_splits(X_train)\n",
        "\n",
        "#TUNING\n",
        "C_values=[100,10,1,0.1, 0.01]\n",
        "Penalties=[\"l2\",\"l1\"]\n",
        "\n",
        "\n",
        "best_accuracy=0\n",
        "best_c=0\n",
        "best_p=0\n",
        " \n",
        "mi=10000 #max_iter\n",
        "max_i=len(C_values)*len(Penalties)*int((1-test_ratio)/val_ratio)\n",
        "\n",
        "i=-1\n",
        "for p in Penalties:\n",
        "    for c in C_values:\n",
        "        clf = LogisticRegression(penalty=p,C=c,solver=\"saga\",max_iter=mi,n_jobs=-1)\n",
        "        mean_accuracies=[]\n",
        "        for train_index, test_index in kf.split(X_train):# KFold Cross Val\n",
        "            i=i+1\n",
        "            percentage=\"Tuning: \"+str(round(100*(i)/max_i,2))+\"%\"\n",
        "            if(best_accuracy>0):\n",
        "              percentage=percentage+\"      best C = \"+str(best_c)+\"      best P = \"+str(best_p)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "            sys.stdout.write('\\r'+percentage)\n",
        "        \n",
        "            X_train_tmp, X_val = X_train[train_index], X_train[test_index]\n",
        "            y_train_tmp, y_val = y_train[train_index], y_train[test_index]\n",
        "            if(smote):\n",
        "              X_train_tmp, y_train_tmp = SMOTE().fit_resample(X_train_tmp, y_train_tmp)\n",
        "            clf.fit(X_train_tmp, y_train_tmp)\n",
        "            y_pred = clf.predict(X_val)\n",
        "            mean_accuracies.append(accuracy_score(y_val,y_pred))\n",
        "        acc=sum(mean_accuracies)/len(mean_accuracies)\n",
        "        if(best_accuracy<acc):\n",
        "            best_accuracy=acc\n",
        "            best_c=c\n",
        "            best_p=p\n",
        "percentage=\"Tuning: 100.0%\"+\"      best C = \"+str(best_c)+\"      best P = \"+str(best_p)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "sys.stdout.write('\\r'+percentage)\n",
        " \n",
        "#TEST\n",
        "if(smote):\n",
        "  X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
        "clf = LogisticRegression(penalty=best_p,C=best_c,solver=\"saga\",max_iter=mi,n_jobs=-1)\n",
        "sys.stdout.write(\"\\n\\nTraining...\")\n",
        "clf.fit(X_train, y_train)\n",
        "sys.stdout.write('\\r'+\"Prediction...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "sys.stdout.write('\\r'+\"Accuracy on test set is \"+str(round(accuracy_score(y_test,y_pred),2)))\n",
        "\n",
        "\n",
        "#PLOT CONFUSION MATRIX\n",
        "print(\"\\n\\nCONFUSION MATRIX\")\n",
        "plot_cm(confusion_matrix(y_test, y_pred),[\"no\",\"yes\"])\n",
        "\n",
        "#ROC CURVE\n",
        "print(\"\\n\\nROC CURVE\")\n",
        "plot_ROC(clf,y_train,y_test)\n",
        "\n",
        "#LEARNING CURVE\n",
        "print(\"\\n\\nLEARNING CURVES\")\n",
        "clf = LogisticRegression(penalty=best_p,C=best_c,solver=\"saga\",max_iter=mi,n_jobs=-1)\n",
        "plot_learning_curve_model(X, y, clf,smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcau2QUGG9tW"
      },
      "source": [
        " K-NEAREST NEIGHBOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PUWQiR-Tj4R"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from imblearn.over_sampling import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import *\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "#LOAD DATASET\n",
        "small=1      #set to 1 if you want to use the small dataset for a faster analysis\n",
        "lenght=1000     #the lenght of small dataset\n",
        "smote=1            #set to 1 if you want SMOTE\n",
        "dataset=getDataset(small,lenght)\n",
        " \n",
        "#ENCODING\n",
        "dataset=encodingDataset(dataset)\n",
        " \n",
        "#TRAIN, VAL e TEST\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, len(dataset.columns)-1].values\n",
        " \n",
        "test_ratio = 0.10\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
        "\n",
        "val_ratio = 0.15\n",
        "kf = KFold(n_splits=int((1-test_ratio)/val_ratio))\n",
        "kf.get_n_splits(X_train)\n",
        "\n",
        "#TUNING\n",
        "accuracies = []\n",
        "K_values=[]\n",
        "for i in range(1,100):\n",
        "  K_values.append(int(i*i*0.75)+1)\n",
        "best_accuracy=0\n",
        "best_k=0\n",
        "\n",
        "max_i=len(K_values)*int((1-test_ratio)/val_ratio)\n",
        "\n",
        "skip=0\n",
        "i=-1\n",
        "for k in K_values:\n",
        "  skip=0\n",
        "  clf = KNeighborsClassifier(n_neighbors=k,n_jobs=-1)\n",
        "  mean_accuracies=[]\n",
        "  for train_index, test_index in kf.split(X_train):# KFold Cross Val\n",
        "    i=i+1\n",
        "    percentage=\"Tuning: \"+str(round(100*(i)/max_i,2))+\"%\"\n",
        "    if(best_accuracy>0):\n",
        "      percentage=percentage+\"      best K = \"+str(best_k)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "    sys.stdout.write('\\r'+percentage)\n",
        "        \n",
        "    X_train_tmp, X_val = X_train[train_index], X_train[test_index]\n",
        "    y_train_tmp, y_val = y_train[train_index], y_train[test_index]\n",
        "    if(smote):\n",
        "      X_train_tmp, y_train_tmp = SMOTE().fit_resample(X_train_tmp, y_train_tmp)\n",
        "    if(len(X_train_tmp)<k):\n",
        "      skip=1\n",
        "      break\n",
        "    clf.fit(X_train_tmp, y_train_tmp)\n",
        "    y_pred = clf.predict(X_val)\n",
        "    mean_accuracies.append(accuracy_score(y_val,y_pred))\n",
        "  if(skip==1):\n",
        "    continue\n",
        "  acc=sum(mean_accuracies)/len(mean_accuracies)\n",
        "  accuracies.append(acc)\n",
        "  if(best_accuracy<acc):\n",
        "      best_accuracy=acc\n",
        "      best_k=k\n",
        "percentage=\"Tuning: 100.0%\"+\"      best K = \"+str(best_k)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "sys.stdout.write('\\r'+percentage)\n",
        "\n",
        "#TEST\n",
        "if(smote):\n",
        "  X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
        "clf = KNeighborsClassifier(n_neighbors=best_k,n_jobs=-1)\n",
        "sys.stdout.write(\"\\n\\nTraining...\")\n",
        "clf.fit(X_train, y_train)\n",
        "sys.stdout.write('\\r'+\"Prediction...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "sys.stdout.write('\\r'+\"Accuracy on test set is \"+str(round(accuracy_score(y_test,y_pred),2)))\n",
        "\n",
        "\n",
        "#PLOT CONFUSION MATRIX\n",
        "print(\"\\n\\nCONFUSION MATRIX\")\n",
        "plot_cm(confusion_matrix(y_test, y_pred),[\"no\",\"yes\"])\n",
        "\n",
        "#ROC CURVE\n",
        "print(\"\\n\\nROC CURVE\")\n",
        "plot_ROC(clf,y_train,y_test)\n",
        "\n",
        "#LEARNING CURVE\n",
        "print(\"\\n\\nLEARNING CURVES\")\n",
        "clf = KNeighborsClassifier(n_neighbors=best_k,n_jobs=-1)\n",
        "plot_learning_curve_model(X, y, clf,smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDxIv_5DU1JD"
      },
      "source": [
        "DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb4FqYzdUSqP"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from imblearn.over_sampling import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import *\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "#LOAD DATASET\n",
        "small=1      #set to 1 if you want to use the small dataset for a faster analysis\n",
        "lenght=1000     #the lenght of small dataset\n",
        "smote=1            #set to 1 if you want SMOTE\n",
        "dataset=getDataset(small,lenght)\n",
        " \n",
        "#ENCODING\n",
        "dataset=encodingDataset(dataset)\n",
        " \n",
        "#TRAIN, VAL e TEST\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, len(dataset.columns)-1].values\n",
        " \n",
        "test_ratio = 0.10\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
        "\n",
        "val_ratio = 0.15\n",
        "kf = KFold(n_splits=int((1-test_ratio)/val_ratio))\n",
        "kf.get_n_splits(X_train)\n",
        "\n",
        "#TUNING\n",
        "Criterions=[\"gini\",\"entropy\"]\n",
        "Max_depth=[15,20,25,50,55,60,65,70,75]\n",
        "min_samples_splits=[4,6,8,10,12,14,16,18,20]\n",
        "\n",
        "best_accuracy=0\n",
        "best_c=0\n",
        "best_d=0\n",
        "best_mss=0\n",
        " \n",
        "max_i=len(Criterions)*len(Max_depth)*len(min_samples_splits)*int((1-test_ratio)/val_ratio)\n",
        "\n",
        "i=-1\n",
        "for c in Criterions:\n",
        "    for d in Max_depth:\n",
        "        for mss in min_samples_splits:\n",
        "            clf = DecisionTreeClassifier(criterion=c,max_depth=d,max_features=\"sqrt\",min_samples_split=mss)\n",
        "            mean_accuracies=[]\n",
        "            for train_index, test_index in kf.split(X_train):# KFold Cross Val\n",
        "                i=i+1\n",
        "                percentage=\"Tuning: \"+str(round(100*(i)/max_i,2))+\"%\"\n",
        "                if(best_accuracy>0):\n",
        "                  percentage=percentage+\"      best C = \"+str(best_c)+\"      best D = \"+str(best_d)+\"      best MSS = \"+str(best_mss)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "                sys.stdout.write('\\r'+percentage)\n",
        "                \n",
        "                X_train_tmp, X_val = X_train[train_index], X_train[test_index]\n",
        "                y_train_tmp, y_val = y_train[train_index], y_train[test_index]\n",
        "                if(smote):\n",
        "                  X_train_tmp, y_train_tmp = SMOTE().fit_resample(X_train_tmp, y_train_tmp)\n",
        "                clf = clf.fit(X_train_tmp, y_train_tmp)\n",
        "                y_pred = clf.predict(X_val)\n",
        "                mean_accuracies.append(accuracy_score(y_val,y_pred))\n",
        "            acc=sum(mean_accuracies)/len(mean_accuracies)\n",
        "            if(best_accuracy<acc):\n",
        "                best_accuracy=acc\n",
        "                best_c=c\n",
        "                best_d=d\n",
        "                best_mss=mss     \n",
        "percentage=\"Tuning: 100.0%\"+\"      best C = \"+str(best_c)+\"      best D = \"+str(best_d)+\"      best MSS = \"+str(best_mss)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "sys.stdout.write('\\r'+percentage)\n",
        "#TEST\n",
        "if(smote):\n",
        "  X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
        "clf = DecisionTreeClassifier(criterion=best_c,max_depth=best_d,max_features=\"sqrt\",min_samples_split=best_mss)\n",
        "sys.stdout.write(\"\\n\\nTraining...\")\n",
        "clf.fit(X_train, y_train)\n",
        "sys.stdout.write('\\r'+\"Prediction...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "sys.stdout.write('\\r'+\"Accuracy on test set is \"+str(round(accuracy_score(y_test,y_pred),2)))\n",
        "\n",
        "\n",
        "#PLOT CONFUSION MATRIX\n",
        "print(\"\\n\\nCONFUSION MATRIX\")\n",
        "plot_cm(confusion_matrix(y_test, y_pred),[\"no\",\"yes\"])\n",
        "\n",
        "#ROC CURVE\n",
        "print(\"\\n\\nROC CURVE\")\n",
        "plot_ROC(clf,y_train,y_test)\n",
        "\n",
        "#PLOT FEATURE IMPORTANCE\n",
        "print(\"\\n\\nFEATURE IMPORTANCE\")\n",
        "plot_FI(clf,num=15,columns=dataset.columns.values)\n",
        "\n",
        "#LEARNING CURVE\n",
        "print(\"\\n\\nLEARNING CURVES\")\n",
        "clf = DecisionTreeClassifier(criterion=best_c,max_depth=best_d,max_features=\"sqrt\",min_samples_split=best_mss)\n",
        "plot_learning_curve_model(X, y, clf,smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "161Wmk1NU_rU"
      },
      "source": [
        "RANDOM FOREST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K3NcSRBU6dG"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from imblearn.over_sampling import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import *\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "#LOAD DATASET\n",
        "small=1      #set to 1 if you want to use the small dataset for a faster analysis\n",
        "lenght=1000     #the lenght of small dataset\n",
        "smote=1            #set to 1 if you want SMOTE\n",
        "dataset=getDataset(small,lenght)\n",
        " \n",
        "#ENCODING\n",
        "dataset=encodingDataset(dataset)\n",
        " \n",
        "#TRAIN, VAL e TEST\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, len(dataset.columns)-1].values\n",
        " \n",
        "test_ratio = 0.10\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
        "\n",
        "val_ratio = 0.15\n",
        "kf = KFold(n_splits=int((1-test_ratio)/val_ratio))\n",
        "kf.get_n_splits(X_train)\n",
        "\n",
        "#TUNING\n",
        "Criterions=[\"gini\",\"entropy\"]\n",
        "Max_depth=[10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
        "min_samples_splits=[2,3,4,5,6,7,8,9]\n",
        "n_estimators=[50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900]\n",
        "\n",
        "best_accuracy=0;\n",
        "best_c=0\n",
        "best_d=0\n",
        "best_mss=0\n",
        "best_n=0\n",
        "\n",
        "if(small):\n",
        "    Criterions=[\"gini\",\"entropy\"]\n",
        "    Max_depth=[50]\n",
        "    min_samples_splits=[5]\n",
        "    n_estimators=[150]\n",
        "\n",
        "max_i=len(Criterions)*len(Max_depth)*len(min_samples_splits)*len(n_estimators)*int((1-test_ratio)/val_ratio)\n",
        "\n",
        "i=-1\n",
        "for c in Criterions:\n",
        "    for d in Max_depth:\n",
        "        for mss in min_samples_splits:\n",
        "            for num in n_estimators:\n",
        "                i=i+1\n",
        "                clf = RandomForestClassifier(n_estimators=num,criterion=c,max_depth=d,max_features=\"sqrt\",min_samples_split=mss,n_jobs=-1)\n",
        "                mean_accuracies=[]\n",
        "                for train_index, test_index in kf.split(X_train):# KFold Cross Val\n",
        "                    i=i+1\n",
        "                    percentage=\"Tuning: \"+str(round(100*(i)/max_i,3))+\"%\"\n",
        "                    if(best_accuracy>0):\n",
        "                      percentage=percentage+\"      best C = \"+str(best_c)+\"      best D = \"+str(best_d)+\"      best MSS = \"+str(best_mss)+\"      best N = \"+str(best_n)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "                    sys.stdout.write('\\r'+percentage)\n",
        "                \n",
        "                    X_train_tmp, X_val = X_train[train_index], X_train[test_index]\n",
        "                    y_train_tmp, y_val = y_train[train_index], y_train[test_index]\n",
        "                    if(smote):\n",
        "                        X_train_tmp, y_train_tmp = SMOTE().fit_resample(X_train_tmp, y_train_tmp)\n",
        "                    clf = clf.fit(X_train_tmp, y_train_tmp)\n",
        "                    y_pred = clf.predict(X_val)\n",
        "                    mean_accuracies.append(accuracy_score(y_val,y_pred))\n",
        "                acc=sum(mean_accuracies)/len(mean_accuracies)\n",
        "                if(best_accuracy<acc):\n",
        "                    best_accuracy=acc\n",
        "                    best_c=c\n",
        "                    best_d=d\n",
        "                    best_mss=mss\n",
        "                    best_n=num\n",
        "\n",
        "percentage=\"Tuning: 100.0%\"+\"      best C = \"+str(best_c)+\"      best D = \"+str(best_d)+\"      best MSS = \"+str(best_mss)+\"      best N = \"+str(best_n)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "sys.stdout.write('\\r'+percentage)\n",
        "#TEST\n",
        "if(smote):\n",
        "  X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
        "clf = RandomForestClassifier(n_estimators=best_n,criterion=best_c,max_depth=best_d,max_features=\"sqrt\",min_samples_split=best_mss,n_jobs=-1)\n",
        "sys.stdout.write(\"\\n\\nTraining...\")\n",
        "clf.fit(X_train, y_train)\n",
        "sys.stdout.write('\\r'+\"Prediction...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "sys.stdout.write('\\r'+\"Accuracy on test set is \"+str(round(accuracy_score(y_test,y_pred),2)))\n",
        "\n",
        "\n",
        "#PLOT CONFUSION MATRIX\n",
        "print(\"\\n\\nCONFUSION MATRIX\")\n",
        "plot_cm(confusion_matrix(y_test, y_pred),[\"no\",\"yes\"])\n",
        "\n",
        "#ROC CURVE\n",
        "print(\"\\n\\nROC CURVE\")\n",
        "plot_ROC(clf,y_train,y_test)\n",
        "\n",
        "#PLOT FEATURE IMPORTANCE\n",
        "print(\"\\n\\nFEATURE IMPORTANCE\")\n",
        "plot_FI(clf,num=15,columns=dataset.columns.values)\n",
        "\n",
        "#LEARNING CURVE\n",
        "print(\"\\n\\nLEARNING CURVES\")\n",
        "clf = RandomForestClassifier(n_estimators=best_n,criterion=best_c,max_depth=best_d,max_features=\"sqrt\",min_samples_split=best_mss,n_jobs=-1)\n",
        "plot_learning_curve_model(X, y, clf,smote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhnXQNxzVT8J"
      },
      "source": [
        "SUPPORT VECTOR MACHINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DWwypvPVTp5"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from imblearn.over_sampling import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import *\n",
        "import warnings\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "gpu=0\n",
        "if(gpu):#INSTALL THUNDERSVM TO RUN IN GPU (about 3 minutes for installation)\n",
        "  !wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "  !ls\n",
        "  !dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "  !ls /var/cuda-repo-9-0-local | grep .pub\n",
        "  !apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "  !apt-get update\n",
        "  !sudo apt-get install cuda-9.0\n",
        "  !nvcc --version\n",
        "  !pip install thundersvm\n",
        "  from thundersvm import SVC\n",
        "else:\n",
        "  from sklearn.svm import SVC\n",
        "\n",
        "#LOAD DATASET\n",
        "small=1      #set to 1 if you want to use the small dataset for a faster analysis\n",
        "lenght=1000     #the lenght of small dataset\n",
        "smote=1            #set to 1 if you want SMOTE\n",
        "dataset=getDataset(small,lenght)\n",
        " \n",
        "#ENCODING\n",
        "dataset=encodingDataset(dataset)\n",
        " \n",
        "#TRAIN, VAL e TEST\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, len(dataset.columns)-1].values\n",
        " \n",
        "test_ratio = 0.10\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
        "\n",
        "val_ratio = 0.15\n",
        "kf = KFold(n_splits=int((1-test_ratio)/val_ratio))\n",
        "kf.get_n_splits(X_train)\n",
        "\n",
        "#TUNING\n",
        "C_values= [ 0.01, 0.1, 1, 10, 100, 1000 ]\n",
        "Gamma_values=[0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "best_accuracy=0\n",
        "best_c=0\n",
        "best_g=0\n",
        " \n",
        "max_i=len(C_values)*len(Gamma_values)*int((1-test_ratio)/val_ratio)\n",
        "\n",
        "i=-1\n",
        "for g in Gamma_values:\n",
        "    for c in C_values:\n",
        "        clf = SVC(C=c, gamma=g)\n",
        "        mean_accuracies=[]\n",
        "        for train_index, test_index in kf.split(X_train):# KFold Cross Val\n",
        "            i=i+1\n",
        "            percentage=\"Tuning: \"+str(round(100*(i)/max_i,2))+\"%\"\n",
        "            if(best_accuracy>0):\n",
        "              percentage=percentage+\"      best C = \"+str(best_c)+\"      best G = \"+str(best_g)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "            sys.stdout.write('\\r'+percentage)\n",
        "        \n",
        "            X_train_tmp, X_val = X_train[train_index], X_train[test_index]\n",
        "            y_train_tmp, y_val = y_train[train_index], y_train[test_index]\n",
        "            if(smote):\n",
        "              X_train_tmp, y_train_tmp = SMOTE().fit_resample(X_train_tmp, y_train_tmp)\n",
        "            clf.fit(X_train_tmp, y_train_tmp)\n",
        "            y_pred = clf.predict(X_val)\n",
        "            mean_accuracies.append(accuracy_score(y_val,y_pred))\n",
        "        acc=sum(mean_accuracies)/len(mean_accuracies)\n",
        "        if(best_accuracy<acc):\n",
        "            best_accuracy=acc\n",
        "            best_c=c\n",
        "            best_g=g\n",
        "percentage=\"Tuning: 100.0%\"+\"      best C = \"+str(best_c)+\"      best G = \"+str(best_g)+\"      best accuracy = \"+str(round(best_accuracy,2))\n",
        "sys.stdout.write('\\r'+percentage)\n",
        " \n",
        "#TEST\n",
        "if(smote):\n",
        "  X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
        "clf = SVC(C=best_c, gamma=best_g,probability=True)\n",
        "sys.stdout.write(\"\\n\\nTraining...\")\n",
        "clf.fit(X_train, y_train)\n",
        "sys.stdout.write('\\r'+\"Prediction...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "sys.stdout.write('\\r'+\"Accuracy on test set is \"+str(round(accuracy_score(y_test,y_pred),2)))\n",
        "\n",
        "\n",
        "#PLOT CONFUSION MATRIX\n",
        "print(\"\\n\\nCONFUSION MATRIX\")\n",
        "plot_cm(confusion_matrix(y_test, y_pred),[\"no\",\"yes\"])\n",
        "\n",
        "#ROC CURVE\n",
        "print(\"\\n\\nROC CURVE\")\n",
        "plot_ROC(clf,y_train,y_test)\n",
        "\n",
        "#LEARNING CURVE\n",
        "print(\"\\n\\nLEARNING CURVES\")\n",
        "clf = SVC(C=best_c, gamma=best_g)\n",
        "plot_learning_curve_model(X, y, clf,smote)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}